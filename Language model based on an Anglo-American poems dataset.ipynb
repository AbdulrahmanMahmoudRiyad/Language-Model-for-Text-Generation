{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7859949,"sourceType":"datasetVersion","datasetId":4610583},{"sourceId":7870361,"sourceType":"datasetVersion","datasetId":4618066},{"sourceId":7870627,"sourceType":"datasetVersion","datasetId":4618250},{"sourceId":7975693,"sourceType":"datasetVersion","datasetId":4693714}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the Language Model\nclass LanguageModel(nn.Module):\n    def __init__(self, vocab_size, n_embd=384, n_head=8, n_layer=8, block_size=256, dropout=0.3):\n        super(LanguageModel, self).__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.ModuleList([Block(n_embd, n_head, dropout) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n        self.block_size = block_size\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n                if module.bias is not None:\n                    nn.init.zeros_(module.bias)\n            elif isinstance(module, nn.Embedding):\n                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)  # (B, T, n_embd)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T, n_embd)\n        x = tok_emb + pos_emb  # (B, T, n_embd)\n        for block in self.blocks:\n            x = block(x)\n        x = self.ln_f(x)  # (B, T, n_embd)\n        logits = self.lm_head(x)  # (B, T, vocab_size)\n\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n            return logits, loss\n        else:\n            return logits\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -self.block_size:]\n            logits = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the Transformer Block\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head, dropout=0.3):\n        super(Block, self).__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size, dropout)\n        self.ffwd = FeedForward(n_embd, dropout)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the Multi-Head Attention Mechanism\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, dropout=0.3):\n        super(MultiHeadAttention, self).__init__()\n        self.heads = nn.ModuleList([Head(head_size, dropout) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a Single Attention Head\nclass Head(nn.Module):\n    def __init__(self, head_size, dropout):\n        super(Head, self).__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)  # (B, T, head_size)\n        q = self.query(x)  # (B, T, head_size)\n        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5  # (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)  # (B, T, head_size)\n        out = wei @ v  # (B, T, head_size)\n        return out\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the FeedForward Network\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd, dropout):\n        super(FeedForward, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to Train the Model\ndef train(model, train_data, val_data, optimizer, max_iters, eval_interval, block_size, device):\n    model.train()\n    for iter in range(max_iters):\n        if iter % eval_interval == 0 or iter == max_iters - 1:\n            losses = evaluate(model, train_data, val_data, block_size, device)\n            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n        xb, yb = get_batch(train_data, block_size, device)\n        logits, loss = model(xb, yb)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to Evaluate the Model\ndef evaluate(model, train_data, val_data, block_size, device):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calculate_loss(model, train_data, block_size, device)\n        val_loss = calculate_loss(model, val_data, block_size, device)\n    model.train()\n    return {'train': train_loss, 'val': val_loss}\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to Calculate Loss\ndef calculate_loss(model, data, block_size, device):\n    losses = []\n    for _ in range(eval_iters):\n        xb, yb = get_batch(data, block_size, device)\n        logits, loss = model(xb, yb)\n        losses.append(loss.item())\n    return sum(losses) / len(losses)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to Get a Batch of Data\ndef get_batch(data, block_size, device):\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i + block_size] for i in ix])\n    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n    return x.to(device), y.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to Decode Token Indices to Text\ndef decode(indices):\n    return ''.join([itos[i] for i in indices])\n\n# Hyperparameters\nbatch_size = 64\nblock_size = 256\nmax_iters = 10000\neval_interval = 500\nlearning_rate = 1e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 8\nn_layer = 8\ndropout = 0.3\n\n# Load Data\nwith open('/kaggle/input/anglo-american-poems/all_data.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\n\n# Train and Test Splits\ndata = torch.tensor([stoi[ch] for ch in text], dtype=torch.long)\nn = int(0.9 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\n# Instantiate Model and Optimizer\nmodel = LanguageModel(vocab_size).to(device)\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n\n# Train the Model\ntrain(model, train_data, val_data, optimizer, max_iters, eval_interval, block_size, device)\n\n# Generate Text from the Trained Model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\ngenerated_text = decode(model.generate(context, max_new_tokens=1000)[0].tolist())\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T17:10:21.743078Z","iopub.execute_input":"2024-05-21T17:10:21.743500Z","iopub.status.idle":"2024-05-21T18:40:18.023157Z","shell.execute_reply.started":"2024-05-21T17:10:21.743473Z","shell.execute_reply":"2024-05-21T18:40:18.022218Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"step 0: train loss 4.6948, val loss 4.7104\nstep 500: train loss 2.2285, val loss 2.3328\nstep 1000: train loss 1.8897, val loss 1.9982\nstep 1500: train loss 1.7067, val loss 1.8170\nstep 2000: train loss 1.6030, val loss 1.7309\nstep 2500: train loss 1.5276, val loss 1.6659\nstep 3000: train loss 1.4767, val loss 1.6279\nstep 3500: train loss 1.4313, val loss 1.5997\nstep 4000: train loss 1.3874, val loss 1.5709\nstep 4500: train loss 1.3584, val loss 1.5510\nstep 5000: train loss 1.3335, val loss 1.5357\nstep 5500: train loss 1.3101, val loss 1.5276\nstep 6000: train loss 1.2900, val loss 1.5181\nstep 6500: train loss 1.2677, val loss 1.5123\nstep 7000: train loss 1.2472, val loss 1.5044\nstep 7500: train loss 1.2318, val loss 1.4988\nstep 8000: train loss 1.2147, val loss 1.4953\nstep 8500: train loss 1.1983, val loss 1.4929\nstep 9000: train loss 1.1819, val loss 1.4913\nstep 9500: train loss 1.1671, val loss 1.4906\nstep 9999: train loss 1.1543, val loss 1.4862\n\n_Before I'd forget my three talked -- \n  Not threated from the kind, dill she grosped\nIn the soundness of the storm \nAnd what you bear the hair new gently.\n\nLike layed by the comfoisomb heart, still, and scarled her\n    and wait come.\nYou and my soul are gay!  I had givenned\nHer boats that blows on and still her love\n    Is coming, glowing, or life-black\n  Socked and remembraned one,\n    That now, sheen my sees of corner walls;\nThere they must stand, and can need\n    where after, they softer bone drift\n    passed sensied, and sherebows in the cree when are\nOn my soul's striking esticial mirals.\n\nFar, roses scene blank, she'd the burn, and tock into each\n    their brathel-cave.\n\n\nColleger Point wind through the forest brink of sacross hell\nIn sleep, or father,\n    She had through me,\nAmid the story thwould saved uncless, circus-Sid, thick wave\n    Wined her\nOffinted the soft paths -- \n    Beckonie Sheer, beneath, and sheep,\nThe Brideblo has helped, wither held hard trees!\n\n\n\nPAIN\nATURE\n\n","output_type":"stream"}]}]}