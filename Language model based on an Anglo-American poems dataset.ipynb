{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the Language Model\n","class LanguageModel(nn.Module):\n","    def __init__(self, vocab_size, n_embd=384, n_head=8, n_layer=8, block_size=256, dropout=0.3):\n","        super(LanguageModel, self).__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.ModuleList([Block(n_embd, n_head, dropout) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd)\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","        self.block_size = block_size\n","\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        for module in self.modules():\n","            if isinstance(module, nn.Linear):\n","                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","                if module.bias is not None:\n","                    nn.init.zeros_(module.bias)\n","            elif isinstance(module, nn.Embedding):\n","                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","        tok_emb = self.token_embedding_table(idx)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n","        x = tok_emb + pos_emb \n","        for block in self.blocks:\n","            x = block(x)\n","        x = self.ln_f(x) \n","        logits = self.lm_head(x)\n","\n","        if targets is not None:\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n","            return logits, loss\n","        else:\n","            return logits\n","\n","    def generate(self, idx, max_new_tokens):\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -self.block_size:]\n","            logits = self(idx_cond)\n","            logits = logits[:, -1, :]\n","            probs = F.softmax(logits, dim=-1)\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","            idx = torch.cat((idx, idx_next), dim=1)\n","        return idx\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the Transformer Block\n","class Block(nn.Module):\n","    def __init__(self, n_embd, n_head, dropout=0.3):\n","        super(Block, self).__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size, dropout)\n","        self.ffwd = FeedForward(n_embd, dropout)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the Multi-Head Attention Mechanism\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, num_heads, head_size, dropout=0.3):\n","        super(MultiHeadAttention, self).__init__()\n","        self.heads = nn.ModuleList([Head(head_size, dropout) for _ in range(num_heads)])\n","        self.proj = nn.Linear(head_size * num_heads, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define a Single Attention Head\n","class Head(nn.Module):\n","    def __init__(self, head_size, dropout):\n","        super(Head, self).__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B, T, C = x.shape\n","        k = self.key(x) \n","        q = self.query(x)\n","        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n","        wei = F.softmax(wei, dim=-1)\n","        wei = self.dropout(wei)\n","        v = self.value(x)\n","        out = wei @ v\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the FeedForward Network\n","class FeedForward(nn.Module):\n","    def __init__(self, n_embd, dropout):\n","        super(FeedForward, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to Train the Model\n","def train(model, train_data, val_data, optimizer, max_iters, eval_interval, block_size, device):\n","    model.train()\n","    for iter in range(max_iters):\n","        if iter % eval_interval == 0 or iter == max_iters - 1:\n","            losses = evaluate(model, train_data, val_data, block_size, device)\n","            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","        xb, yb = get_batch(train_data, block_size, device)\n","        logits, loss = model(xb, yb)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to Evaluate the Model\n","def evaluate(model, train_data, val_data, block_size, device):\n","    model.eval()\n","    with torch.no_grad():\n","        train_loss = calculate_loss(model, train_data, block_size, device)\n","        val_loss = calculate_loss(model, val_data, block_size, device)\n","    model.train()\n","    return {'train': train_loss, 'val': val_loss}\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to Calculate Loss\n","def calculate_loss(model, data, block_size, device):\n","    losses = []\n","    for _ in range(eval_iters):\n","        xb, yb = get_batch(data, block_size, device)\n","        logits, loss = model(xb, yb)\n","        losses.append(loss.item())\n","    return sum(losses) / len(losses)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to Get a Batch of Data\n","def get_batch(data, block_size, device):\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i + block_size] for i in ix])\n","    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n","    return x.to(device), y.to(device)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T17:10:21.743500Z","iopub.status.busy":"2024-05-21T17:10:21.743078Z","iopub.status.idle":"2024-05-21T18:40:18.023157Z","shell.execute_reply":"2024-05-21T18:40:18.022218Z","shell.execute_reply.started":"2024-05-21T17:10:21.743473Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["step 0: train loss 4.6948, val loss 4.7104\n","step 500: train loss 2.2285, val loss 2.3328\n","step 1000: train loss 1.8897, val loss 1.9982\n","step 1500: train loss 1.7067, val loss 1.8170\n","step 2000: train loss 1.6030, val loss 1.7309\n","step 2500: train loss 1.5276, val loss 1.6659\n","step 3000: train loss 1.4767, val loss 1.6279\n","step 3500: train loss 1.4313, val loss 1.5997\n","step 4000: train loss 1.3874, val loss 1.5709\n","step 4500: train loss 1.3584, val loss 1.5510\n","step 5000: train loss 1.3335, val loss 1.5357\n","step 5500: train loss 1.3101, val loss 1.5276\n","step 6000: train loss 1.2900, val loss 1.5181\n","step 6500: train loss 1.2677, val loss 1.5123\n","step 7000: train loss 1.2472, val loss 1.5044\n","step 7500: train loss 1.2318, val loss 1.4988\n","step 8000: train loss 1.2147, val loss 1.4953\n","step 8500: train loss 1.1983, val loss 1.4929\n","step 9000: train loss 1.1819, val loss 1.4913\n","step 9500: train loss 1.1671, val loss 1.4906\n","step 9999: train loss 1.1543, val loss 1.4862\n","\n","_Before I'd forget my three talked -- \n","  Not threated from the kind, dill she grosped\n","In the soundness of the storm \n","And what you bear the hair new gently.\n","\n","Like layed by the comfoisomb heart, still, and scarled her\n","    and wait come.\n","You and my soul are gay!  I had givenned\n","Her boats that blows on and still her love\n","    Is coming, glowing, or life-black\n","  Socked and remembraned one,\n","    That now, sheen my sees of corner walls;\n","There they must stand, and can need\n","    where after, they softer bone drift\n","    passed sensied, and sherebows in the cree when are\n","On my soul's striking esticial mirals.\n","\n","Far, roses scene blank, she'd the burn, and tock into each\n","    their brathel-cave.\n","\n","\n","Colleger Point wind through the forest brink of sacross hell\n","In sleep, or father,\n","    She had through me,\n","Amid the story thwould saved uncless, circus-Sid, thick wave\n","    Wined her\n","Offinted the soft paths -- \n","    Beckonie Sheer, beneath, and sheep,\n","The Brideblo has helped, wither held hard trees!\n","\n","\n","\n","PAIN\n","ATURE\n","\n"]}],"source":["# Function to Decode Token Indices to Text\n","def decode(indices):\n","    return ''.join([itos[i] for i in indices])\n","\n","# Hyperparameters\n","batch_size = 64\n","block_size = 256\n","max_iters = 10000\n","eval_interval = 500\n","learning_rate = 1e-4\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embd = 384\n","n_head = 8\n","n_layer = 8\n","dropout = 0.3\n","\n","# Load Data\n","with open('/kaggle/input/anglo-american-poems/all_data.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","stoi = {ch: i for i, ch in enumerate(chars)}\n","itos = {i: ch for i, ch in enumerate(chars)}\n","\n","# Train and Test Splits\n","data = torch.tensor([stoi[ch] for ch in text], dtype=torch.long)\n","n = int(0.9 * len(data))\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# Instantiate Model and Optimizer\n","model = LanguageModel(vocab_size).to(device)\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","# Train the Model\n","train(model, train_data, val_data, optimizer, max_iters, eval_interval, block_size, device)\n","\n","# Generate Text from the Trained Model\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","generated_text = decode(model.generate(context, max_new_tokens=1000)[0].tolist())\n","print(generated_text)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4610583,"sourceId":7859949,"sourceType":"datasetVersion"},{"datasetId":4618066,"sourceId":7870361,"sourceType":"datasetVersion"},{"datasetId":4618250,"sourceId":7870627,"sourceType":"datasetVersion"},{"datasetId":4693714,"sourceId":7975693,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
